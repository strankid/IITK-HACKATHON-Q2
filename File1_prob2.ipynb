{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os, sys\n",
    "import base64\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import binascii\n",
    "from elftools.elf.elffile import ELFFile, ELFError\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pickle import dump\n",
    "\n",
    "random.seed(a=1234567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a file and parse the text section of elf file into a string\n",
    "# returns empty string when there is no text section, or file is not a valid elf file\n",
    "def process_file(label, label_list, Binaries_list, filename, file_sections = ['.text', '.comment']):\n",
    "    out_array = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        try:\n",
    "            elffile = ELFFile(f)\n",
    "            section_list = {}\n",
    "            for section in elffile.iter_sections():\n",
    "                section_list[section.name] = section.data()\n",
    "            \n",
    "            for file_section in file_sections:\n",
    "                try:\n",
    "                    fbuf = section_list[file_section]\n",
    "                    out_array.append(fbuf.hex())\n",
    "                except:\n",
    "                    out_array.append('')\n",
    "            \n",
    "            Binaries_list.append(out_array)  \n",
    "            label_list.append(label)\n",
    "            return \n",
    "        except ELFError:\n",
    "            print(\"Error opening file\")\n",
    "            return 'NOTELF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Strings for pre-final data\n",
    "benign_dir_path = '/Users/gurjotsingh/Downloads/malware_training_data/benign_ELF/'\n",
    "#benign_dir_path_esc = benign_dir_path.replace(\" \", \"\\ \")\n",
    "malware_dir_path = '/Users/gurjotsingh/Downloads/malware_training_data/malware_ELF/'\n",
    "#malware_dir_path_esc = malware_dir_path.replace(\" \", \"\\ \")\n",
    "\n",
    "\n",
    "# Structures for Data extraction\n",
    "Binaries_list = [] \n",
    "distinct_labels = []\n",
    "label_list = []\n",
    "num_duplicates = 0\n",
    "meta_path = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign 1915\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "DDoS 661\n",
      "Error opening file\n",
      "Error opening file\n",
      "Backdoor 617\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Trojan 408\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Botnet 639\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Virus 452\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n",
      "Error opening file\n"
     ]
    }
   ],
   "source": [
    "# load all types of malware in separate arrays\n",
    "malware_dirs = [f for f in listdir(malware_dir_path)]\n",
    "\n",
    "# labels\n",
    "distinct_labels = distinct_labels + malware_dirs + ['Benign']\n",
    "\n",
    "# load all benign files names\n",
    "benign_file_names = [benign_dir_path + f for f in listdir(benign_dir_path) if isfile(join(benign_dir_path, f))]\n",
    "\n",
    "# add benign file paths to\n",
    "meta_path['Benign'] = benign_file_names\n",
    "\n",
    "#extract relevant sections\n",
    "extract_sections = [\".text\",\".comment\",\".note\", \".symtab\", \".strtab\", \".rodata\",'.init']\n",
    "\n",
    "# load all malware file names and \n",
    "for i, dir in enumerate(malware_dirs):\n",
    "    meta_path[dir] = [malware_dir_path + malware_dirs[i] + '/' + \n",
    "                     f for f in listdir(malware_dir_path+malware_dirs[i])\n",
    "                        if isfile(join(malware_dir_path+malware_dirs[i], f))]\n",
    "\n",
    "# make byte level array for text section of each file\n",
    "for label in meta_path.keys():\n",
    "    print(label, len(meta_path[label]))\n",
    "    label_file_path = meta_path[label]\n",
    "    for path in label_file_path:\n",
    "        process_file(label, label_list, Binaries_list, path, extract_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Benign': 1886, 'DDoS': 659, 'Backdoor': 603, 'Botnet': 586, 'Virus': 435, 'Trojan': 373})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binaries_df = pd.DataFrame(Binaries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Info:  dict_values([1886, 659, 603, 373, 586, 435])\n",
      "Distinct Labels: ['DDoS', 'Backdoor', 'Trojan', 'Botnet', 'Virus', 'Benign']\n",
      "Total Unique Data points: 4542\n",
      "Number of Duplicates: 0\n",
      "Minimum number of data points in a class: 373\n",
      "Number of distinct datasets possible: 1\n",
      "Number of expected data points per class 180\n",
      "binaries length:  4542\n",
      "Labels length:  4542\n",
      "len(Binaries_DataSets_train):  3406\n",
      "len(Binaries_DataSets_test):  1136\n",
      "len(Binaries_LabelSets_train):  3406\n",
      "len(Binaries_LabelSets_test):  1136\n"
     ]
    }
   ],
   "source": [
    "label_info = Counter(label_list).values()\n",
    "print('Label Info: ', label_info)\n",
    "print('Distinct Labels:', distinct_labels)\n",
    "#print('Label list: ', label_list)\n",
    "print('Total Unique Data points:', sum(label_info))\n",
    "print('Number of Duplicates:', num_duplicates)\n",
    "\n",
    "min_len =  min(Counter(label_list).values()) #Minimum number of data points per class\n",
    "print('Minimum number of data points in a class:', min_len)\n",
    "\n",
    "#Convert string labels to integers \n",
    "label_list_numeric = []\n",
    "for ii in range(0, len(label_list)):\n",
    "    label_index = distinct_labels.index(label_list[ii])\n",
    "    label_list_numeric.append(label_index)\n",
    "\n",
    "class_data_info = []\n",
    "#picked_data_indices = []\n",
    "for ii in range(0, len(distinct_labels)):\n",
    "    class_data_info.append([])\n",
    "    #picked_data_indices.append([])\n",
    "\n",
    "#Collecting indices belong to each class    \n",
    "for ii in range(0, len(label_list)):\n",
    "    class_data_info[label_list_numeric[ii]].append(ii)\n",
    "\n",
    "    \n",
    "#num_distinct_datasets = sum(label_info)//4008\n",
    "train_dps_per_class = 144\n",
    "test_dps_per_class = 36\n",
    "num_expected_dps_per_class = train_dps_per_class + test_dps_per_class\n",
    "num_distinct_datasets = 1\n",
    "#num_distinct_datasets = 2\n",
    "\n",
    "print('Number of distinct datasets possible:', num_distinct_datasets)\n",
    "print('Number of expected data points per class', num_expected_dps_per_class)\n",
    "print('binaries length: ', len(Binaries_list))\n",
    "print('Labels length: ', len(label_list))\n",
    "\n",
    "Binaries_DataSets_train, Binaries_DataSets_test, Binaries_LabelSets_train, Binaries_LabelSets_test = train_test_split(Binaries_df, label_list)\n",
    "\n",
    "print('len(Binaries_DataSets_train): ', len(Binaries_DataSets_train))\n",
    "print('len(Binaries_DataSets_test): ', len(Binaries_DataSets_test))\n",
    "print('len(Binaries_LabelSets_train): ', len(Binaries_LabelSets_train))\n",
    "print('len(Binaries_LabelSets_test): ', len(Binaries_LabelSets_test))\n",
    "\n",
    "with open(\"DistinctLabels.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(distinct_labels, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF N-Gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_train = 'Binaries_DataSets_train'\n",
    "with open(file_name_train, 'wb') as f:\n",
    "    pickle.dump(Binaries_DataSets_train, f)\n",
    "\n",
    "file_name_test = 'Binaries_DataSets_test'\n",
    "with open(file_name_test, 'wb') as f:\n",
    "    pickle.dump(Binaries_DataSets_test, f)\n",
    "    \n",
    "file_name_train = 'Binaries_LabelSets_train'\n",
    "with open(file_name_train, 'wb') as f:\n",
    "    pickle.dump(Binaries_LabelSets_train, f)\n",
    "\n",
    "file_name_test = 'Binaries_LabelSets_test'\n",
    "with open(file_name_test, 'wb') as f:\n",
    "    pickle.dump(Binaries_LabelSets_test, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = 75   \n",
    "df_train = pd.DataFrame(index=np.arange(Binaries_DataSets_train.shape[0]),columns=np.arange(max_features*Binaries_DataSets_train.shape[1] ))    \n",
    "df_test = pd.DataFrame(index=np.arange(Binaries_DataSets_test.shape[0]),columns=np.arange(max_features*Binaries_DataSets_test.shape[1] ))   \n",
    "cumulator=0\n",
    "\n",
    "for i in range(Binaries_DataSets_train.shape[1]):\n",
    "    vectorizer_3 = TfidfVectorizer(ngram_range = (1,3), max_features = max_features,lowercase = False, analyzer='char')\n",
    "    #fit TF-IDF model using training data and transform training data according to the fitted model \n",
    "    X_train = vectorizer_3.fit_transform(Binaries_DataSets_train[i])\n",
    "    No_Columns = pd.DataFrame(X_train.toarray()).shape[1]\n",
    "    df_train.iloc[: , cumulator:cumulator + No_Columns] = pd.DataFrame(X_train.toarray())\n",
    "    \n",
    "    #transform test data using the fitted model\n",
    "    X_test = vectorizer_3.transform(Binaries_DataSets_test[i])\n",
    "    df_test.iloc[: , cumulator:cumulator + No_Columns] = pd.DataFrame(X_test.toarray())\n",
    "    \n",
    "    cumulator += No_Columns\n",
    "    \n",
    "    dump(vectorizer_3, open('trans-'+extract_sections[i]+'.pkl', 'wb'))\n",
    "\n",
    "df_train.drop(df_train.columns.to_series()[cumulator:], axis=1, inplace=True)\n",
    "df_test.drop(df_test.columns.to_series()[cumulator:], axis=1, inplace=True)\n",
    "\n",
    "df_train['label'] = Binaries_LabelSets_train\n",
    "df_test['label'] = Binaries_LabelSets_test\n",
    "\n",
    "df_train_name = 'BinaryBlobData_3Gram-TrainSet'\n",
    "df_train.to_pickle(df_train_name)\n",
    "df_test.to_csv(df_train_name+\".csv\")\n",
    "\n",
    "df_test_name = 'BinaryBlobData_3Gram-TestSet'\n",
    "df_test.to_pickle(df_test_name)\n",
    "df_test.to_csv(df_test_name+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.506563</td>\n",
       "      <td>0.158906</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.036185</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>0.00827</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.00702</td>\n",
       "      <td>0.022981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218472</td>\n",
       "      <td>0.152947</td>\n",
       "      <td>0.064736</td>\n",
       "      <td>0.06574</td>\n",
       "      <td>0.072528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776913</td>\n",
       "      <td>0.248151</td>\n",
       "      <td>0.102097</td>\n",
       "      <td>0.00587</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.048785</td>\n",
       "      <td>0.019333</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>0.038869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273338</td>\n",
       "      <td>0.278223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.461821</td>\n",
       "      <td>0.301072</td>\n",
       "      <td>Botnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.737796</td>\n",
       "      <td>0.49849</td>\n",
       "      <td>0.334775</td>\n",
       "      <td>0.019062</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.035255</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.03724</td>\n",
       "      <td>0.021475</td>\n",
       "      <td>0.030274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.026553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149855</td>\n",
       "      <td>0.091947</td>\n",
       "      <td>Backdoor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.612405</td>\n",
       "      <td>0.355976</td>\n",
       "      <td>0.250971</td>\n",
       "      <td>0.030575</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.013391</td>\n",
       "      <td>0.051799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059427</td>\n",
       "      <td>0.06043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071295</td>\n",
       "      <td>0.041593</td>\n",
       "      <td>0.052814</td>\n",
       "      <td>0.053633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790175</td>\n",
       "      <td>0.384125</td>\n",
       "      <td>0.169479</td>\n",
       "      <td>0.032044</td>\n",
       "      <td>0.02686</td>\n",
       "      <td>0.077384</td>\n",
       "      <td>0.03292</td>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.043736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038215</td>\n",
       "      <td>0.17504</td>\n",
       "      <td>0.034039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3401</th>\n",
       "      <td>0.639284</td>\n",
       "      <td>0.381291</td>\n",
       "      <td>0.259547</td>\n",
       "      <td>0.031753</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.036066</td>\n",
       "      <td>0.031409</td>\n",
       "      <td>0.02285</td>\n",
       "      <td>0.019083</td>\n",
       "      <td>0.042658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104624</td>\n",
       "      <td>0.106388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125517</td>\n",
       "      <td>0.073226</td>\n",
       "      <td>0.09298</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>0.629901</td>\n",
       "      <td>0.288886</td>\n",
       "      <td>0.191522</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.034313</td>\n",
       "      <td>0.031157</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.01252</td>\n",
       "      <td>0.017141</td>\n",
       "      <td>0.033753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039384</td>\n",
       "      <td>0.120264</td>\n",
       "      <td>0.105243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14972</td>\n",
       "      <td>0.065071</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403</th>\n",
       "      <td>0.583543</td>\n",
       "      <td>0.300195</td>\n",
       "      <td>0.208972</td>\n",
       "      <td>0.041722</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.028732</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>0.074791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102561</td>\n",
       "      <td>0.104291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.071783</td>\n",
       "      <td>0.091147</td>\n",
       "      <td>0.092561</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.133147</td>\n",
       "      <td>Virus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>0.658666</td>\n",
       "      <td>0.314726</td>\n",
       "      <td>0.216913</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.016101</td>\n",
       "      <td>0.011272</td>\n",
       "      <td>0.026435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078719</td>\n",
       "      <td>0.150235</td>\n",
       "      <td>0.105176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Botnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>0.630858</td>\n",
       "      <td>0.376583</td>\n",
       "      <td>0.26393</td>\n",
       "      <td>0.033157</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.009886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013173</td>\n",
       "      <td>0.036216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067983</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.051141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3406 rows × 484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.506563  0.158906  0.096983  0.004603  0.036185  0.014058   0.00827   \n",
       "1     0.776913  0.248151  0.102097   0.00587  0.008865  0.048785  0.019333   \n",
       "2     0.737796   0.49849  0.334775  0.019062  0.005499  0.035255  0.017152   \n",
       "3     0.612405  0.355976  0.250971  0.030575  0.011596  0.030612  0.020274   \n",
       "4     0.790175  0.384125  0.169479  0.032044   0.02686  0.077384   0.03292   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3401  0.639284  0.381291  0.259547  0.031753  0.004244  0.036066  0.031409   \n",
       "3402  0.629901  0.288886  0.191522  0.003881  0.034313  0.031157  0.010931   \n",
       "3403  0.583543  0.300195  0.208972  0.041722  0.006849  0.028732  0.015814   \n",
       "3404  0.658666  0.314726  0.216913  0.003739  0.036219  0.022373  0.010476   \n",
       "3405  0.630858  0.376583   0.26393  0.033157  0.003296  0.009886       0.0   \n",
       "\n",
       "             7         8         9  ...       474       475       476  \\\n",
       "0     0.011009   0.00702  0.022981  ...       0.0       0.0       0.0   \n",
       "1     0.031271  0.030895  0.038869  ...       0.0       0.0  0.273338   \n",
       "2      0.03724  0.021475  0.030274  ...  0.008853       0.0  0.013913   \n",
       "3     0.011532  0.013391  0.051799  ...  0.059427   0.06043       0.0   \n",
       "4     0.097904   0.02208  0.043736  ...       0.0       0.0  0.038215   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3401   0.02285  0.019083  0.042658  ...  0.104624  0.106388       0.0   \n",
       "3402   0.01252  0.017141  0.033753  ...       0.0       0.0  0.039384   \n",
       "3403  0.008621  0.008765  0.074791  ...  0.102561  0.104291       0.0   \n",
       "3404  0.016101  0.011272  0.026435  ...       0.0       0.0  0.078719   \n",
       "3405       0.0  0.013173  0.036216  ...  0.056667  0.057623       0.0   \n",
       "\n",
       "           477       478       479       480       481       482     label  \n",
       "0     0.218472  0.152947  0.064736   0.06574  0.072528       0.0     Virus  \n",
       "1     0.278223       0.0       0.0       0.0  0.461821  0.301072    Botnet  \n",
       "2     0.026553       0.0       0.0       0.0  0.149855  0.091947  Backdoor  \n",
       "3     0.071295  0.041593  0.052814  0.053633       0.0       0.0    Benign  \n",
       "4      0.17504  0.034039       0.0       0.0  0.435821       0.0    Trojan  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3401  0.125517  0.073226   0.09298  0.094422       0.0       0.0    Benign  \n",
       "3402  0.120264  0.105243       0.0       0.0   0.14972  0.065071      DDoS  \n",
       "3403  0.123042  0.071783  0.091147  0.092561  0.102119  0.133147     Virus  \n",
       "3404  0.150235  0.105176       0.0       0.0       0.0       0.0    Botnet  \n",
       "3405  0.067983  0.039661   0.05036  0.051141       0.0       0.0    Benign  \n",
       "\n",
       "[3406 rows x 484 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.57751</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.57719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.638206</td>\n",
       "      <td>0.39012</td>\n",
       "      <td>0.276454</td>\n",
       "      <td>0.030793</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.029044</td>\n",
       "      <td>0.022886</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>0.016364</td>\n",
       "      <td>0.054142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060323</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>0.094797</td>\n",
       "      <td>0.108553</td>\n",
       "      <td>0.04222</td>\n",
       "      <td>0.053609</td>\n",
       "      <td>0.054441</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.612908</td>\n",
       "      <td>0.36678</td>\n",
       "      <td>0.271867</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.02141</td>\n",
       "      <td>0.011873</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.00823</td>\n",
       "      <td>0.067466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057142</td>\n",
       "      <td>0.058105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068553</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.050782</td>\n",
       "      <td>0.05157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599128</td>\n",
       "      <td>0.345023</td>\n",
       "      <td>0.24478</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.01432</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.050077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057372</td>\n",
       "      <td>0.05834</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.068829</td>\n",
       "      <td>0.040155</td>\n",
       "      <td>0.050987</td>\n",
       "      <td>0.051778</td>\n",
       "      <td>0.057125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.561915</td>\n",
       "      <td>0.257127</td>\n",
       "      <td>0.182469</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.029568</td>\n",
       "      <td>0.033529</td>\n",
       "      <td>0.015756</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.011934</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192587</td>\n",
       "      <td>0.134826</td>\n",
       "      <td>0.057066</td>\n",
       "      <td>0.057951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.312415</td>\n",
       "      <td>0.222664</td>\n",
       "      <td>0.035486</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.021352</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.067076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058601</td>\n",
       "      <td>0.059589</td>\n",
       "      <td>0.046046</td>\n",
       "      <td>0.105454</td>\n",
       "      <td>0.041015</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>0.052887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>0.573214</td>\n",
       "      <td>0.332221</td>\n",
       "      <td>0.228735</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.018649</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.00562</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.058606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059649</td>\n",
       "      <td>0.060655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07156</td>\n",
       "      <td>0.041748</td>\n",
       "      <td>0.053011</td>\n",
       "      <td>0.053833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>0.528933</td>\n",
       "      <td>0.206484</td>\n",
       "      <td>0.130698</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>0.035177</td>\n",
       "      <td>0.022126</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>0.013025</td>\n",
       "      <td>0.022154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182226</td>\n",
       "      <td>0.127572</td>\n",
       "      <td>0.053996</td>\n",
       "      <td>0.054833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>0.528556</td>\n",
       "      <td>0.160488</td>\n",
       "      <td>0.101002</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.022204</td>\n",
       "      <td>0.007187</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>0.052608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>0.134645</td>\n",
       "      <td>0.089773</td>\n",
       "      <td>0.028498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trojan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>0.577388</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.577313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1136 rows × 484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0      0.57751   0.57735   0.57719       0.0       0.0       0.0       0.0   \n",
       "1     0.638206   0.39012  0.276454  0.030793  0.012869  0.029044  0.022886   \n",
       "2     0.612908   0.36678  0.271867  0.036458  0.003295   0.02141  0.011873   \n",
       "3     0.599128  0.345023   0.24478  0.028818  0.002387   0.01432  0.002458   \n",
       "4     0.561915  0.257127  0.182469  0.002624  0.029568  0.033529  0.015756   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1131    0.5605  0.312415  0.222664  0.035486  0.004675  0.021352  0.011378   \n",
       "1132  0.573214  0.332221  0.228735  0.035779  0.018649  0.020371   0.00562   \n",
       "1133  0.528933  0.206484  0.130698  0.003739  0.035177  0.022126  0.012163   \n",
       "1134  0.528556  0.160488  0.101002  0.012128  0.012691  0.022204  0.007187   \n",
       "1135  0.577388   0.57735  0.577313       0.0       0.0       0.0       0.0   \n",
       "\n",
       "             7         8         9  ...       474       475       476  \\\n",
       "0          0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "1     0.007198  0.016364  0.054142  ...  0.060323   0.06134  0.094797   \n",
       "2     0.001843   0.00823  0.067466  ...  0.057142  0.058105       0.0   \n",
       "3          0.0  0.014312  0.050077  ...  0.057372   0.05834  0.045081   \n",
       "4     0.014856  0.011934  0.031802  ...       0.0       0.0       0.0   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1131  0.009391  0.015288  0.067076  ...  0.058601  0.059589  0.046046   \n",
       "1132  0.009159  0.005635  0.058606  ...  0.059649  0.060655       0.0   \n",
       "1133  0.016068  0.013025  0.022154  ...       0.0       0.0       0.0   \n",
       "1134  0.010648  0.005706  0.052608  ...       0.0       0.0  0.025196   \n",
       "1135       0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "\n",
       "           477       478       479       480       481  482   label  \n",
       "0          0.0       0.0       0.0       0.0       0.0  0.0   Virus  \n",
       "1     0.108553   0.04222  0.053609  0.054441       0.0  0.0  Benign  \n",
       "2     0.068553  0.039993  0.050782   0.05157       0.0  0.0  Benign  \n",
       "3     0.068829  0.040155  0.050987  0.051778  0.057125  0.0  Benign  \n",
       "4     0.192587  0.134826  0.057066  0.057951       0.0  0.0  Trojan  \n",
       "...        ...       ...       ...       ...       ...  ...     ...  \n",
       "1131  0.105454  0.041015  0.052079  0.052887       0.0  0.0  Benign  \n",
       "1132   0.07156  0.041748  0.053011  0.053833       0.0  0.0  Benign  \n",
       "1133  0.182226  0.127572  0.053996  0.054833       0.0  0.0  Trojan  \n",
       "1134  0.134645  0.089773  0.028498       0.0  0.063856  0.0  Trojan  \n",
       "1135       0.0       0.0       0.0       0.0       0.0  0.0   Virus  \n",
       "\n",
       "[1136 rows x 484 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification Report Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, \\\n",
    "precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_m2m_curve(ax, xs, ys, x_metric, y_metric, title):\n",
    "    \"\"\" Helper function for plotting metric-to-metric curve. \"\"\"\n",
    "    # Plot metric-to-metric curve\n",
    "    ax.plot(xs, ys)\n",
    "    \n",
    "    # Make sub-plot square\n",
    "    ax.set_aspect(\"equal\")\n",
    "    # Set axes labels\n",
    "    ax.set_xlabel(x_metric)\n",
    "    ax.set_ylabel(y_metric)\n",
    "    # Set title\n",
    "    ax.set_title(title)\n",
    "def evaluate_model(model, name, feat_test, y_test):\n",
    "    \"\"\" Evaluate a classification model on the test set, then print and plot metrics. \"\"\"\n",
    "    # Make prediction from features\n",
    "    pred_test = model.predict(feat_test)\n",
    "    \n",
    "    print(f\"\\n[ Evaluation result for {name} ]\")\n",
    "    # Print classification report\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BinaryBlob dataset and corresponding distinct labels (Types of Architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment in progres ...\n",
      "df_test.shape:  (1136, 484)\n",
      "Total Number of Training data samples: 3406\n",
      "Total Number of Testing data samples: 1136\n",
      "Number of features per Train data sample: 483\n",
      "Number of features per Test data sample: 483\n",
      "Distinct labels (Architecture Types):\n",
      " ['DDoS', 'Backdoor', 'Trojan', 'Botnet', 'Virus', 'Benign']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "random.seed(a= 1234567)\n",
    "\n",
    "\n",
    "print('Experiment in progres ...')\n",
    "\n",
    "#Combining (1,2)-gram and 3-gram features to make the complete Training set \n",
    "df_train = pd.read_pickle('BinaryBlobData_3Gram-TrainSet')\n",
    "\n",
    "#Combining (1,2)-gram and 3-gram features to make the complete Test set \n",
    "df_test = pd.read_pickle('BinaryBlobData_3Gram-TestSet')\n",
    "\n",
    "print(\"df_test.shape: \", df_test.shape)\n",
    "\n",
    "\n",
    "## Visualize the dataset in tabular format\n",
    "print('Total Number of Training data samples:', df_train.shape[0])\n",
    "print('Total Number of Testing data samples:', df_test.shape[0])\n",
    "print('Number of features per Train data sample:', df_train.shape[1]-1)\n",
    "print('Number of features per Test data sample:', df_test.shape[1]-1)\n",
    "print('Distinct labels (Architecture Types):\\n',distinct_labels)\n",
    "\n",
    "## Converting labels  to integer values\n",
    "\n",
    "def label_to_numeric(row):\n",
    "    \"\"\"Convert label to integers\"\"\"\n",
    "    index_ = distinct_labels.index(row[\"label\"])\n",
    "    return index_\n",
    "\n",
    "df_train[\"label\"] = df_train.apply(label_to_numeric, axis=1)\n",
    "df_test[\"label\"] = df_test.apply(label_to_numeric, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Labels: 6\n",
      "Labels: ['DDoS', 'Backdoor', 'Trojan', 'Botnet', 'Virus', 'Benign']\n",
      "\n",
      "[ Evaluation result for Decision Tree Classifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.83       160\n",
      "           1       0.75      0.67      0.70       150\n",
      "           2       0.41      0.63      0.50       105\n",
      "           3       0.84      0.71      0.77       153\n",
      "           4       0.60      0.50      0.55       111\n",
      "           5       0.98      0.96      0.97       457\n",
      "\n",
      "    accuracy                           0.80      1136\n",
      "   macro avg       0.73      0.72      0.72      1136\n",
      "weighted avg       0.82      0.80      0.80      1136\n",
      "\n",
      "\n",
      "[ Evaluation result for Random Forest Classifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       160\n",
      "           1       0.87      0.73      0.79       150\n",
      "           2       0.51      0.60      0.55       105\n",
      "           3       0.84      0.85      0.85       153\n",
      "           4       0.68      0.70      0.69       111\n",
      "           5       0.99      0.98      0.98       457\n",
      "\n",
      "    accuracy                           0.86      1136\n",
      "   macro avg       0.80      0.80      0.80      1136\n",
      "weighted avg       0.87      0.86      0.86      1136\n",
      "\n",
      "[18:52:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "[ Evaluation result for XGBoost Classifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       160\n",
      "           1       0.85      0.75      0.79       150\n",
      "           2       0.47      0.62      0.53       105\n",
      "           3       0.88      0.84      0.86       153\n",
      "           4       0.67      0.61      0.64       111\n",
      "           5       0.98      0.98      0.98       457\n",
      "\n",
      "    accuracy                           0.85      1136\n",
      "   macro avg       0.80      0.79      0.79      1136\n",
      "weighted avg       0.86      0.85      0.86      1136\n",
      "\n",
      "\n",
      "[ Evaluation result for K NearestNeighbors Classifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.86       160\n",
      "           1       0.71      0.69      0.70       150\n",
      "           2       0.48      0.65      0.55       105\n",
      "           3       0.83      0.73      0.77       153\n",
      "           4       0.67      0.53      0.59       111\n",
      "           5       0.98      0.97      0.98       457\n",
      "\n",
      "    accuracy                           0.82      1136\n",
      "   macro avg       0.75      0.74      0.74      1136\n",
      "weighted avg       0.83      0.82      0.82      1136\n",
      "\n",
      "Experiment  Complete !!!\n"
     ]
    }
   ],
   "source": [
    "## Removing randomly choosen class of data from the dataset and varying training data and evaluating accuracy for fixed (25%) test data\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import copy \n",
    "\n",
    "label_numerals = list(range(0,len(distinct_labels)))\n",
    "\n",
    "df_train_tmp = copy.deepcopy(df_train)\n",
    "df_test_tmp = copy.deepcopy(df_test)\n",
    "\n",
    "print('Number of Labels:', len(label_numerals))\n",
    "current_labels = [val for kk, val in enumerate(distinct_labels) if kk in label_numerals]\n",
    "print('Labels:', current_labels)\n",
    "\n",
    "#Train and Test labels\n",
    "y_train = df_train_tmp.pop(\"label\").values\n",
    "y_test = df_test_tmp.pop(\"label\").values\n",
    "\n",
    "#Train and Test data\n",
    "X_train = df_train_tmp.values\n",
    "X_test = df_test_tmp.values\n",
    "\n",
    "\n",
    "#DT\n",
    "DT_model = DecisionTreeClassifier(random_state=1234567)\n",
    "DT_model.fit(X_train, y_train)\n",
    "evaluate_model(DT_model, \"Decision Tree Classifier\", X_test, y_test) \n",
    "DT_acc = np.round(DT_model.score(X_test, y_test)*100,2)\n",
    "\n",
    "#RF\n",
    "RF_model = RandomForestClassifier(random_state=1234567)\n",
    "RF_model.fit(X_train, y_train)\n",
    "evaluate_model(RF_model, \"Random Forest Classifier\", X_test, y_test) \n",
    "RF_acc = np.round(RF_model.score(X_test, y_test)*100,2)\n",
    "\n",
    "#XGB\n",
    "XGB_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=1234567, use_label_encoder=False, n_estimators = 100)\n",
    "XGB_model.fit(X_train, y_train)\n",
    "evaluate_model(XGB_model, \"XGBoost Classifier\", X_test, y_test) \n",
    "XGB_acc = np.round(XGB_model.score(X_test, y_test)*100,2)\n",
    "\n",
    "#KNN\n",
    "KNN_model = KNeighborsClassifier()\n",
    "KNN_model.fit(X_train, y_train)\n",
    "evaluate_model(KNN_model, \"K NearestNeighbors Classifier\", X_test, y_test)\n",
    "KNN_acc = np.round(KNN_model.score(X_test, y_test)*100,2)\n",
    "\n",
    "print('Experiment  Complete !!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Evaluation result for MLPClassifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       160\n",
      "           1       0.59      0.89      0.71       150\n",
      "           2       0.65      0.59      0.62       105\n",
      "           3       0.93      0.73      0.82       153\n",
      "           4       0.70      0.47      0.56       111\n",
      "           5       0.97      0.98      0.97       457\n",
      "\n",
      "    accuracy                           0.84      1136\n",
      "   macro avg       0.79      0.76      0.77      1136\n",
      "weighted avg       0.85      0.84      0.84      1136\n",
      "\n",
      "Experiment Complete !!!\n"
     ]
    }
   ],
   "source": [
    "#MLPClassifier\n",
    "MLPC_model = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300, activation = 'relu', solver='adam', random_state=1234567)\n",
    "#MLPC_model = MLPClassifier(hidden_layer_sizes=(150,150,100,100,50,50), max_iter=500, activation = 'relu', solver='adam', random_state=1234567)\n",
    "MLPC_model.fit(X_train, y_train)\n",
    "evaluate_model(MLPC_model, \"MLPClassifier\", X_test, y_test)\n",
    "MLPC_acc = np.round(MLPC_model.score(X_test, y_test)*100,2)\n",
    "\n",
    "print('Experiment Complete !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:57:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:57:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:58:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:58:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:59:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:59:38] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "[ Evaluation result for Stacking Classifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       160\n",
      "           1       0.86      0.76      0.81       150\n",
      "           2       0.52      0.63      0.57       105\n",
      "           3       0.88      0.85      0.86       153\n",
      "           4       0.68      0.67      0.67       111\n",
      "           5       0.98      0.98      0.98       457\n",
      "\n",
      "    accuracy                           0.86      1136\n",
      "   macro avg       0.81      0.80      0.80      1136\n",
      "weighted avg       0.87      0.86      0.87      1136\n",
      "\n",
      "\n",
      "[ Evaluation result for Bagging Classifier ]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       160\n",
      "           1       0.86      0.75      0.80       150\n",
      "           2       0.51      0.62      0.56       105\n",
      "           3       0.90      0.84      0.87       153\n",
      "           4       0.63      0.70      0.67       111\n",
      "           5       0.99      0.97      0.98       457\n",
      "\n",
      "    accuracy                           0.86      1136\n",
      "   macro avg       0.80      0.80      0.80      1136\n",
      "weighted avg       0.87      0.86      0.86      1136\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8591549295774648"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [('RF', RandomForestClassifier(random_state=1234567)),\n",
    "              ('XGB', xgb.XGBClassifier(objective=\"multi:softprob\", random_state=1234567, use_label_encoder=False, n_estimators = 100)),\n",
    "              ('KNN', KNeighborsClassifier())]\n",
    "\n",
    "Stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(random_state=123))\n",
    "Stacking_model.fit(X_train, y_train)\n",
    "evaluate_model(Stacking_model, \"Stacking Classifier\", X_test, y_test) \n",
    "Stacking_model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging_model = BaggingClassifier(random_state = 123, n_estimators = 100)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "evaluate_model(bagging_model, \"Bagging Classifier\", X_test, y_test) #100 desicion trees\n",
    "bagging_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
